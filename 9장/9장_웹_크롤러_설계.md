## 서론

### 웹 크롤러란 무엇인가?

- 위키피디아에 따르면 “**웹 크롤러**(web crawler)는 조직적, 자동화된 방법으로 월드 와이드 웹을 탐색하는 컴퓨터 프로그램이다.”
- 탐색을 하는 이유는 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내기 위해서이다.
  (콘텐츠는 웹 페이지에 추가로 이미지나 비디오, 또는 PDF 파일도 포함한다.)

### 작동 방식

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지 다운로드
2. 다운받은 웹 페이지에서 URL들을 추출
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 1번부터 계속해서 반복

### 이용 사례

1. 검색 엔진 인덱싱(Search engine indexing)

   흔히들 구글에서 많이 들었을 SEO가 이 방식을 이용해서 최적화를 진행하는 것이다.

   구글 공식 가이드 문서에 이러한 설명이 쓰여져있는 것을 확인할 수 있다.


![Untitled](https://private-user-images.githubusercontent.com/106607029/351752879-7b55c16a-241e-4b51-9d32-a23eea36e0a6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjE4MzEzODgsIm5iZiI6MTcyMTgzMTA4OCwicGF0aCI6Ii8xMDY2MDcwMjkvMzUxNzUyODc5LTdiNTVjMTZhLTI0MWUtNGI1MS05ZDMyLWEyM2VlYTM2ZTBhNi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzI0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcyNFQxNDI0NDhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kMDQ0YjFhZmUxNGEwYjE5MmVjMjA1MzNkZDk5OTJkOTU3OWMwZWJlMmU4YjZmNWFjMmYzOTI5ZWEwNmI4MWNiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.AV1DzDwQOa8forjgtq1S_8MIRjdYOwFMOUugaEf-54c)

1. 웹 아카이빙(Web Archiving)

   나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차이다. 많은 국립 도서관이 이 방식을 사용 중이다.

2. 웹 마이닝(Web Mining)

   데이터 마이닝 기술을 응용한 것으로 웹에서 유용한 지식을 캐내는 것으로 간단하게 이해 가능하다.

3. 웹 모니터링(Web Monitoring)

   저작권이나 상표권 침해 사례를 모니터링 하기 위해 사용된다.


<aside>
💡 이제 웹 크롤링이 무엇이고 어디에 활용될 수 있는 프로그램인지를 알아봤으니 면접 문제를 예시로 자세히 알아보자.

</aside>

## 본론(웹 크롤러 설계 면접)

### 조건 제시

- 해당 면접에서 우리는 면접관에게 조건을 제시 받아 웹 크롤러를 설계할 예정이다.
- 조건은 아래와 같다.

```jsx
1. 검색 엔진 인덱싱에 쓰일 예정이다.
2. 매달 10억 개의 웹 페이지를 수집할 것이다.
3. 웹 페이지를 수집하며 새로 만들어지거나 수정된 웹 페이지도 고려해서 설계하라.
4. 총 저장 기간은 5년이다. 
5. 중복된 컨텐츠는 무시한다.
```

### 설계 단계

- 우리는 좋은 웹 크롤러를 만들어야 면접에 합격할테니 최소한 지켜야 할 부분은 짚어보자.

```jsx
1. 규모 확장성 - 병렬로 웹 크롤러 프로그램을 운용하면 성능상 효과적이다.
2. 안정성 - 웹은 위험하다. 위험 요소는 피해서 관리해야 한다.
3. 예절 - 웹 크롤러는 요청을 보내는 것이므로 많은 요청을 한 번에 보내서는 안된다.
4. 확장성 - 어떤 설계든 확장을 고려하는 것은 당연하다. 
```

- 지켜야 할 내용을 마음에 새겼다면 이제 대략적인 계산을 진행하자.

    ```jsx
    QPS(Query Per Second) = 10억(개) / 30 (일) / 24 (시간) / 60 (분) / 60 (초) 
                          = 약 400 페이지
    페이지 한 개 500 KB 가정 시, 10억 * 500 = 500TB/월
    -> 500 TB * 12(개월) * 5 (년) = 30 PB
    ```

  초 당 400페이지를 접근해서 저장해야 하고 5년 기준 30 PB 크기의 저장공간이 필요하다.


### 설계안

![Untitled](https://private-user-images.githubusercontent.com/106607029/351753755-32902393-8c9f-48ce-863f-86fa7d6bd29c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjE4MzE1MDcsIm5iZiI6MTcyMTgzMTIwNywicGF0aCI6Ii8xMDY2MDcwMjkvMzUxNzUzNzU1LTMyOTAyMzkzLThjOWYtNDhjZS04NjNmLTg2ZmE3ZDZiZDI5Yy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzI0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcyNFQxNDI2NDdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zNDkzOTIyZTk2OGVkNmI1OTU3YTJlODIwZmY1NTc0N2E0YTJhZmQzN2I1MzE2Yjk4ZDNlYTEwNTdjMjEzNTQ4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.5kKkYGbQNLzaNNhO6qsQOah8TLUErs6yQDNf4_9f0WQ)

각각 파트에서 어떤 일을 하는지 간략하게 설명하며 연관 관계를 순서대로 보고 지나가겠다.

1. 시작 URL 집합 : 말 그대로 크롤러가 탐색을 시작할 URL 집합이다.
2. 미수집 URL 저장소 : 중복을 줄이기 위해 다운로드 할 “예정”인 URL만 따로 모은다. 대체로 큐를 활용한다.
3. 도메인 이름 변환기 : URL을 IP 주소로 변환하기 위한 작업을 수행한다.
4. HTML 다운로더 : 웹페이지 내용을 다운로드 받는다.
5. 컨텐츠 파서 : 내용물을 확인한다. 처음에 말했던 안정성을 유지하기 위해 있는 컴포넌트로 이상한 웹페이지를 거른다.
6. 중복 컨텐츠 : 파서에서 넘겼을 때 이미 시스템에 저장된 콘텐츠인지 알아낸다.
7. 컨텐츠 저장소 : 대부분 컨텐츠의 저장 공간이다. 디스크에 저장할 것이며 인기 검색어 위주로 메모리로 빼낸다.
8. URL 추출기 : 파싱한 페이지에서 연결되는 URL들을 추출해서 뽑아낸다.
9. URL 필터 : 이상한 URL은 쳐낸다. 예를 들자면 마이페이지나 결제창 같은 프라이빗하거나 캐낼 것이 없는 곳은 쳐내는 것이 맞다.
10. 이미 방문한 URL : URL 저장소에 이미 방문한 URL 들을 저장해놓고 중복한 건 빼고 큐에 넣어준다.

<aside>
💡 중복 컨텐츠
이해가 안가서 별도로 검색해봤는데 웹 사이트 여러 개가 동일한 컨텐츠를 포함하고 있는 경우로 쉽게 예시를 들자면 주로 블로그 글을 읽을 때 많이 볼 수 있는 광고들이 중복 컨텐츠가 될 수 있지 않을까 싶다.

</aside>

### 세부 설계 - 최적화

1. BFS

   웹 크롤러는 일반적으로 BFS 알고리즘을 이용해서 탐색한다. DFS는 얼마나 깊이까지 URL 연결된 것을 탐색 해야할지 정하는 것이 어렵기 때문이다.

   하지만 BFS에도 문제점은 존재한다.

   첫째, 병렬적으로 처리해서 진행할 경우 같은 호스트에 속한 많은 링크를 접근하느랴 “예의”를 지키지 못할 것이다.

   둘째, 여러 웹 사이트를 탐색할 때 각 웹 페이지들의 중요성은 각각 다르지만 BFS 방식은 모두를 공평하게 대우하는 것이다.

2. 미수집 URL 저장소

   해당 방식은 BFS의 문제점을 해결 가능하다.

   웹 사이트의 호스트명과 다운로드를 수행할 작업 스레드 간의 관계를 유지해서 같은 페이지를 다운받는 작업을 시간 차를 두고 해결하도록 하면 된다.

   ![Untitled](https://private-user-images.githubusercontent.com/106607029/351754049-318abca9-7a6f-4fb8-b37e-4be5c391912c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjE4MzE1NDgsIm5iZiI6MTcyMTgzMTI0OCwicGF0aCI6Ii8xMDY2MDcwMjkvMzUxNzU0MDQ5LTMxOGFiY2E5LTdhNmYtNGZiOC1iMzdlLTRiZTVjMzkxOTEyYy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzI0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcyNFQxNDI3MjhaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01MmMwMTRjYmM5YzBkMDVkNjIyZTIzZDUxODc1ZWJhYWIzMjcwYzYzYzZjYjljOThjNDkwZDJiOWY4NjkyYjBkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.C4yGG7zt9qNYZ6tZPG0UO_wzhhBrbxTZCN5I1Ypytd4)

   위 처럼 큐 라우터는 같은 호스트는 같은 큐로 들어가도록 배분해준다. 이러면 같은 호스트는 하나의 큐에서 순서대로 나올 수 밖에 없으므로 시간 차를 두고 하나씩 처리할 수 있도록 도와준다.

   ![Untitled](https://private-user-images.githubusercontent.com/106607029/351754282-6bb5ab49-3ffb-45a3-9a66-f40706c92000.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjE4MzE1ODQsIm5iZiI6MTcyMTgzMTI4NCwicGF0aCI6Ii8xMDY2MDcwMjkvMzUxNzU0MjgyLTZiYjVhYjQ5LTNmZmItNDVhMy05YTY2LWY0MDcwNmM5MjAwMC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzI0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcyNFQxNDI4MDRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kNmVmMDBlMTBkNWRkNmUyOWI3MzcxOTJiM2QxMmFkZTU5ODZlNGRlOWRhM2FjMmVlYmUxYjExZDdmYTgxNzAwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.LAtIFtn53TsQSQoLHMqHVvzzj4ZPoonIaUJAHeQXyLY)

   우선 순위를 결정하는 것은 상단 큐 라우터 이전에 설치하여 우선 순위가 높은 것을 먼저 접근할 수 있도록 한다.

   예를 들어, 애플 공식 홈페이지와 관련 기사를 다루는 홈페이지가 있다면 애플 공식 홈페이지를 더 우선적으로 다뤄야하므로 우선순위를 높게 책정하는 방식이다.

   이 방식을 이용해 우선순위가 높은 홈페이지는 더 수시로 업데이트 내용을 체크하는 방법도 활용 가능하다.

    1. HTML 다운로더 (Main Server 느낌)

       미수집 URL 저장소에서 URL을 모두 나누어 놨으므로 HTML 다운로더에서는 여러 가지 성능 상 이점을 가져갈 수 있도록 설계 할 수 있다.

       첫 째, 다운로더 서버를 여러 개로 분산 시켜 진행한다. 이미 URL은 미수집 URL 저장소에서 분류에 따라 나누어 놨기 때문에 각 서버는 나눠진 URL을 각각 맡아 처리하면 된다.

       둘 째, DNS 변환기로 인한 성능 저하 해결을 위해 한 번 조회한 IP 주소는 캐시에 매핑해서 저장하여 효과적으로 높일 수 있다.

       셋 째, 크롤링 작업을 수행하는 서버를 대상 서버와 가까운 곳으로 배정해서 처리하도록 하는 것이다. 지역적으로 서버가 가까이 위치한다면 다운로드에 걸리는 시간을 절약할 수 있다.

       마지막으로, 짧은 타임아웃을 설정해서 오래 걸리는 웹 페이지는 과감하게 다운로드를 중지하는 방법이 있다.


### 세부설계 - 안정성

안정성은 안정 해시를 이용한 방식으로 해결할 수 있다. (5장 정리 내용을 참고하자.)

### 문제 있는 컨텐츠 감지 및 회피 방법

1. 중복 컨텐츠

   해시나 체크섬을 사용하면 중복 컨텐츠를 쉽게 탐지할 수 있다.

   여담이지만, 해시는 데이터가 조금이라도 달라지면 바로 알아챌 수 있지만 체크섬을 이용한 체크를 진행할 때는 여러 데이터를 수정해 결과값을 맞출 수 있어서 해시보다는 안정성이 떨어진다고 합니다.

2. 거미 덫

   기이할 정도로 긴 URL을 가진 웹사이트들이다. 예를 들어 thousandSunny/bing/q/kjy/leemw/bing/….이런 식으로 계속 가는 것이다.

   덫을 자동으로 피하는 알고리즘을 만들어내는 것은 어려워서 직접 확인하고 찾아내서 수동으로 탐색 대상에서 제외시키고 필터 목록에 추가하는 식으로 해결해야한다.

3. 데이터 노이즈

   광고나 스팸 같은 것들은 필터를 이용해서 걸러내야한다.


## 결론

웹 크롤러는 생각보다 복잡하다. 구글에서는 Index 작업을 통해 웹 페이지 컨텐츠를 분석하고 분류해서 구글을 사용하는 사람들에게 더 나은 검색 환경을 제공하기 위해 노력하고 있다고 한다.

[https://www.jics.or.kr/digital-library/manuscript/file/15722/05_나철원.pdf](https://www.jics.or.kr/digital-library/manuscript/file/15722/05_%EB%82%98%EC%B2%A0%EC%9B%90.pdf) 해당 논문은 웹 크롤러에 관한 것으로 시간이 남거나 할 게 너무 없다 싶은 사람은 한 번 쯤 읽어보면 좋을 것 같다. (물론 나도 못 읽었다.)